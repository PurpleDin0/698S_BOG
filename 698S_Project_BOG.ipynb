{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "698S-Project-BOG.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "7-5zfYftLLdP",
        "YF3l0PTHtaML",
        "bOaYwt1_Oveb",
        "94g9T_L1Ju85"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PurpleDin0/698S_BOG/blob/master/698S_Project_BOG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAZDZ5_d4gsK",
        "colab_type": "text"
      },
      "source": [
        "# Web Scraper Execution Notebook\n",
        "**BLUF:** THis notebook imports the relevant python scraping scripts from github, builds the envirnoment, executes the scripts to scrape a selected website's files, saves the data locally, allows a user to export the data, then performs basic NLP on the content of the files to rank the files.\n",
        "\n",
        "* [x] The target websites are located here [Link to website list](https://drive.google.com/file/d/1y8FL1rBu8yb2HEn7-ok-ILy1Wq3UrDZU/view?usp=sharing).  Note: website list restricted, for access see the author.\n",
        "* [x] The desired search terms are stored in this file [Link to search terms list](https://drive.google.com/file/d/18MyoEiAYTG6w6JRejv5d4qVK4RxYyRu2/view?usp=sharing).  Note: file access restricted, for access see the author.\n",
        "\n",
        "**Problem Statement:**\n",
        "Builds a tool to scrape websites and then analyze the information to see if it contains key terms.  Provide the analyst with a rank-ordered list of documents that require human review. \n",
        "1. [x] Construct a Python scripts that can be used to scrape the pdfs from a set of [websites](https://drive.google.com/file/d/1y8FL1rBu8yb2HEn7-ok-ILy1Wq3UrDZU/view?usp=sharing).  \n",
        "2. [x] Construct a python Notebook that executes the python script and then performs natural language processing (NLP) to the scrapped data.  \n",
        "  - [x] 2.1. Build the initial section that executes the python scripts.\n",
        "  - [x] 2.2. Rank order the scrapped results by relevance to the analyist. The ranking will be based on the [list of key terms](https://drive.google.com/file/d/18MyoEiAYTG6w6JRejv5d4qVK4RxYyRu2/view?usp=sharing).\n",
        "  - [x] 2.3. include an ability to automate the translation of a selected file and perform sentiment analysis\n",
        "portions of the scraped content.\n",
        "  - [x] 2.4. Provide the user with the ability to save the scrapped content.\n",
        "\n",
        "- Other possible things: \n",
        "  - [x] The code should allow the list of keywords to change over time.  \n",
        "  -  The system  should immediately flag certain keywords for human review regardless of other content in the document.  \n",
        "  -  The system should be able to retrieve and analyze office documents (PDFs,\n",
        "spreadsheets, and word processor documents). \n",
        "  - [x] The system should perform sentiment analysis of selected content.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F7jL-UyzwOd",
        "colab_type": "text"
      },
      "source": [
        "# Verify a clean environment, and that external IP is different from your own.\n",
        " Check yours [here](https://www.google.com/search?q=whatmyip)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WjiaE_VxbnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import socket\n",
        "import subprocess\n",
        "\n",
        "hostname = socket.gethostname()\n",
        "internal_ip = socket.gethostbyname(hostname)\n",
        "external_ip = subprocess.check_output('curl ifconfig.me', shell=True)\n",
        "print('~'*35)\n",
        "print(\"Colab VM Hostname: {}\".format(hostname))\n",
        "print(\"Colab Internal IP: {}\".format(internal_ip))\n",
        "print('Colab External IP: {}'.format(external_ip.decode('utf-8')))\n",
        "print('~'*35)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGpNrVuaWeHo",
        "colab_type": "text"
      },
      "source": [
        "# Prepare the environment \n",
        "0. git the repo and install all dependancies\n",
        "1. Install any dependancies\n",
        "2. Load the list of search sites and search terms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6czqjNjv5IN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "## 0. Clone the repo and then change the working directory to the cloned folder\n",
        "root_folder = '/content' # Root folder for storing the files\n",
        "git_repo =  '698S_BOG'  # Repo we want to clone\n",
        "git_user = 'PurpleDin0' # User/org we want to clone the repo from\n",
        "os.chdir(root_folder)\n",
        "!git clone https://github.com/{git_user}/{git_repo}.git\n",
        "repo_dir = os.path.join(os.getcwd(), git_repo)\n",
        "os.chdir(repo_dir)\n",
        "\n",
        "## 1. Install the required libraries\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Caution: Colab may tell you that you need to restart, \n",
        "# But as of 22 June 2020 you don't have to, the code still works even with the different versions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsReszSfBH1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 2. Load the list of search sites and search terms\n",
        "\n",
        "# Import PyDrive and associated libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Download a file based on its file ID.\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "# \n",
        "# Get the website list file and load it to a dataframe\n",
        "# Website list file is a csv with the columns [Website, Domain, FQD, notes]\n",
        "file_id = '1y8FL1rBu8yb2HEn7-ok-ILy1Wq3UrDZU'  ## USER UPDATEABLE VALUE! ##\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.FetchMetadata()\n",
        "filename = downloaded.metadata['originalFilename']\n",
        "downloaded.GetContentFile(filename)\n",
        "website_list_df = pd.read_csv(filename)\n",
        "# Get the search term file and load it to a dataframe\n",
        "# Search term file is a csv with columns [Importance,Term_(foreign),Term_(english)]\n",
        "file_id = '18MyoEiAYTG6w6JRejv5d4qVK4RxYyRu2' ## USER UPDATEABLE VALUE! ##\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.FetchMetadata()\n",
        "filename = downloaded.metadata['originalFilename'] \n",
        "downloaded.GetContentFile(filename)\n",
        "search_terms_df = pd.read_csv(filename)\n",
        "\n",
        "\n",
        "# Cool banner printer\n",
        "def banner(t, s='~'):\n",
        "    l = s * (len(t) + 4)\n",
        "    print(l + '\\n' + '{0} {1} {0}'.format(s, t) + '\\n' + l)\n",
        "\n",
        "# Formatted line\n",
        "def line(t):\n",
        "  print(\"\\n\", \"~\"*(26-(len(t)//2)), t, \"~\"*(26-(len(t)//2)))\n",
        "\n",
        "banner('Colab environment configured')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb-e4zNwbTPJ",
        "colab_type": "text"
      },
      "source": [
        "# Run the spider web crawler script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn4BgfagrPrP",
        "colab_type": "text"
      },
      "source": [
        "Menu for selecting the site to crawl."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m-QAgg_jbA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ALLOWED = ''\n",
        "URL = ''\n",
        "def menu():\n",
        "  banner(\"Select a site to scrape\")\n",
        "  for i, site in enumerate(website_list_df['FQD']):\n",
        "    print('['+str(i)+'] '+ site)\n",
        "  opt = input('Select a site by #:\\n ')\n",
        "  try:\n",
        "    opt = int(opt)\n",
        "    if opt in range(0, website_list_df['FQD'].count()):\n",
        "      global ALLOWED\n",
        "      global URL\n",
        "      ALLOWED = website_list_df['Domain'][opt]\n",
        "      URL = website_list_df['FQD'][opt]\n",
        "      line('Selected: {}'.format(URL))\n",
        "    else:\n",
        "      print('Input must be in range.')\n",
        "      menu()\n",
        "  except Exception as e:\n",
        "    print('Input must be a number.')\n",
        "    menu()   \n",
        "\n",
        "menu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQM5OB3QLegR",
        "colab_type": "text"
      },
      "source": [
        "## Execute the script that gets the links\n",
        "\n",
        "- run this to execute the webcrawler script that gets the links from the target webpage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fvkh65rjQR48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 sixnineeight.py ALLOWED_DOMAINS={ALLOWED} URLS={URL} # This is a custom spider class, see script for custom configs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU6cJcqxtqkL",
        "colab_type": "text"
      },
      "source": [
        "# Download the files and create a zip file for processing\n",
        "- run this to scrape the pdfs from the target webpage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqTejgOEdYSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from urllib.parse import urlparse, urljoin\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Prints a banner\n",
        "banner('Beginning file downloads')\n",
        "\n",
        "# Sets custom user-agent string to masquerade \n",
        "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36'}\n",
        "\n",
        "# Reads the links fro the json file and sorts for .pdf (or other) filetypes found\n",
        "pdf_links = pd.read_json('/content/698S_BOG/links.json')\n",
        "pdf_links = pdf_links[pdf_links['link'].str.contains(\".pdf\", na=False)] \n",
        "\n",
        "# Loops through the list of links\n",
        "for url in pdf_links.link.values.tolist():\n",
        "  # Gets the basename of the file and if a dynamic link sets a new name\n",
        "  filename = os.path.basename(urlparse(url).path)\n",
        "  if os.path.exists(os.path.join('content', filename)):\n",
        "    filename = filename + urlparse(url).query + '.pdf'\n",
        "  \n",
        "  print('*'*50)\n",
        "  print(\"Requesting: \" + url)\n",
        "  try:\n",
        "    # Checks if content directory exists, if not, create\n",
        "    if not os.path.exists('content'):\n",
        "      os.makedirs('content')\n",
        "\n",
        "    # Makes the request\n",
        "    r = requests.get(url, stream=True, headers=headers)\n",
        "\n",
        "    # Processes if link is valid, or raise generic exception\n",
        "    if r.status_code == 200:\n",
        "      # Chunks the file for progress bar\n",
        "      total_size = int(r.headers.get('content-length', 0))\n",
        "      block_size = 1024 \n",
        "\n",
        "      # Progress bar parameters\n",
        "      t=tqdm(total=total_size, unit='iB', unit_scale=True)\n",
        "      \n",
        "      # Downloads the files to the content folder\n",
        "      with open(os.path.join('content', filename), 'wb') as f:\n",
        "          for data in r.iter_content(block_size):\n",
        "              t.update(len(data))\n",
        "              f.write(data)\n",
        "      \n",
        "      # Error handling\n",
        "      if total_size != 0 and t.n != total_size:\n",
        "        raise\n",
        "      \n",
        "      # Close the progress bar object\n",
        "      t.close()\n",
        "    else:\n",
        "      print('{} Response, Skipping {}.'.format(r.status_code, url))\n",
        "    \n",
        "  except Exception as e:\n",
        "    print(\"Error: {}\".format(e))\n",
        "\n",
        "# Zip up the files\n",
        "banner(\"Zipping up the files\")\n",
        "\n",
        "# Zip object walks directory and packs all files without directory structure\n",
        "try:\n",
        "  ziper = zipfile.ZipFile('content.zip', 'w', zipfile.ZIP_DEFLATED)\n",
        "  for root, dirs, files in os.walk('content/'):\n",
        "    for file in files:\n",
        "      ziper.write(os.path.join(root, file), file)\n",
        "  ziper.close()\n",
        "except Exception as e:\n",
        "  print(\"Could not zip the file. \" + e)\n",
        "\n",
        "# Cleans up the files and removes the content directory\n",
        "try:\n",
        "  if os.path.exists('content'):\n",
        "    for root, dirs, files in os.walk('content/'):\n",
        "      for file in tqdm(files):\n",
        "        os.remove(os.path.join(root, file))\n",
        "    os.removedirs('content')\n",
        "  if os.path.exists('content.zip'):\n",
        "    print('\\nZip file saved')\n",
        "  else:\n",
        "    raise Exception(\"Could not write zip file.\")\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuzvEVZ_VdLm",
        "colab_type": "text"
      },
      "source": [
        "# Parse the documents for the desired text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iox-jtkBcq3n",
        "colab_type": "text"
      },
      "source": [
        "## Import parsing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nay_fjEHcp5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import packages needed form parsing\n",
        "import os\n",
        "import pandas as pd\n",
        "import tika\n",
        "tika.initVM()\n",
        "from tika import parser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8jr_PMIc1-X",
        "colab_type": "text"
      },
      "source": [
        "## Load the search files and search terms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqFJGQTwO5Iq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# My take on unzipping the files again\n",
        "import zipfile\n",
        "\n",
        "working_folder = os.path.join('/content', '698S_BOG')  ## DEBUG ## hard_coded\n",
        "filename = os.path.join(working_folder, 'content.zip')\n",
        "scrape_folder = os.path.join(working_folder, 'scraped_files')\n",
        "\n",
        "try:\n",
        "  if not os.path.exists(scrape_folder):\n",
        "    os.makedirs(scrape_folder)\n",
        "\n",
        "  with zipfile.ZipFile(filename, 'r') as z:\n",
        "    z.extractall(scrape_folder)\n",
        "    \n",
        "  os.chdir(scrape_folder)\n",
        "  line(\"Files unzipped\")\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSWu-hWXcpJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the search term file and load it to a dataframe\n",
        "os.chdir(os.path.join('/content', git_repo))\n",
        "file_id = '18MyoEiAYTG6w6JRejv5d4qVK4RxYyRu2'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.FetchMetadata()\n",
        "filename = downloaded.metadata['originalFilename']\n",
        "downloaded.GetContentFile(filename)\n",
        "search_terms_df = pd.read_csv(filename)\n",
        "\n",
        "os.chdir(scrape_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLj4Mv-DKPgH",
        "colab_type": "text"
      },
      "source": [
        "## Open the PDFs and parse the contents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWYHCgjJdzVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## open all the PDFs and parse their content into dataframes\n",
        "pdf_df = pd.DataFrame()\n",
        "for filename in os.listdir(scrape_folder):\n",
        "    if filename.endswith(\".pdf\"):\n",
        "        parsed = parser.from_file(filename)\n",
        "        row = pd.DataFrame(\n",
        "            [[filename, scrape_folder, parsed[\"metadata\"], parsed[\"content\"]]],\n",
        "            index = [filename],\n",
        "            columns = ['filename', 'folder', 'metadata', 'content'])\n",
        "        pdf_df = pdf_df.append(row)\n",
        "\n",
        "# Replace any NaN values from the parser with ''\n",
        "pdf_df = pdf_df.fillna('') \n",
        "\n",
        "# Cound the number of occurances of each term in the content of each document\n",
        "for term in search_terms_df['Term_(foreign)']:\n",
        "    pdf_df[term] = pdf_df['content'].apply(lambda x : x.count(term))\n",
        "\n",
        "## Calculate the relative importance of each search term based on the importance values stored in the search_terms.csv\n",
        "\n",
        "# Function to compute the relative importance of a value in a list of importance values\n",
        "# takes an importance val and list of importance values and returns a relative importance scaler\n",
        "def compute_rel_importance(val, importance_vals):\n",
        "  importance_vals.sort()\n",
        "  importance_val_sum = sum(importance_vals)\n",
        "  x = 0\n",
        "  rel_importance = importance_val_sum\n",
        "  while val != importance_vals[x]:\n",
        "      x += 1\n",
        "      rel_importance = rel_importance - importance_vals[x]\n",
        "  return rel_importance\n",
        "\n",
        "# Calculate and add a relative importance column to the search terms dataframe\n",
        "importance_vals = list(dict.fromkeys(search_terms_df['Importance']))\n",
        "search_terms_df['Relative_Importance'] = search_terms_df['Importance'].apply(lambda x : compute_rel_importance(x, importance_vals))\n",
        "\n",
        "## Compute the relative importance of each file based on the (occurance*relative importance) of each search term \n",
        "\n",
        "for term in search_terms_df['Term_(foreign)']:\n",
        "    term_score = term + '_score'\n",
        "    pdf_df[term+'_score'] = pdf_df[term].apply(lambda x : x * search_terms_df.loc[search_terms_df['Term_(foreign)'] == term, 'Relative_Importance'])\n",
        "col_to_sum = [item for item in list(pdf_df.columns) if any(sub in item for sub in ['score'])] \n",
        "pdf_df['importance_rank_score'] = pdf_df[col_to_sum].sum(axis=1)\n",
        "pdf_df = pdf_df.drop(col_to_sum, axis=1)\n",
        "pdf_df = pdf_df.sort_values(by='importance_rank_score', ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbIdhfXnO430",
        "colab_type": "text"
      },
      "source": [
        "### Show the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeswSbWHO4Rp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Display the whole dataframe\n",
        "pdf\n",
        "\n",
        "## Optionally, you can show just search term results by uncommenting the below two lines \n",
        "# view = list(search_terms_df['Term_(foreign)']) + ['importance_rank_score']\n",
        "# pdf_df[view]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ-9ExG1kfRG",
        "colab_type": "text"
      },
      "source": [
        "## Translate the text content of a selected document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyZ9qa6JkduK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the unofficial google translate package\n",
        "# This package allows us to call the google translate website for small ~1500 char translations\n",
        "import googletrans\n",
        "from googletrans import Translator\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ho34wYBDaBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Filename that you want to translate into english\n",
        "# If you don't change it, it defaults to the highest ranked file\n",
        "filename_to_translate = pdf_df.iloc[0].filename ## Replace this with the filename you want to translate\n",
        "\n",
        "# Load the content of the desired file from the pandas dataframe\n",
        "content_to_translate = pdf_df.loc[filename_to_translate].content\n",
        "\n",
        "# clean any excessive line breaks (i.e. double) from the document\n",
        "content_to_translate = content_to_translate.replace('\\n\\n', ' ')\n",
        "\n",
        "# instantiate the translator object\n",
        "translator = Translator()\n",
        "\n",
        "\n",
        "# Check the length of the content, if it is more then 1500 batch the requests to translate\n",
        "if len(content_to_translate) < 1500: \n",
        "  # Detect the language of the text\n",
        "  detected_language = translator.detect(content_to_translate)\n",
        "\n",
        "  # Translate the text and store the text output to 'translated_content'\n",
        "  translated_batch = translator.translate(content_to_translate, dest = 'en', src = detected_language.lang)\n",
        "  translated_content = translated_batch.text\n",
        "else:\n",
        "  # Initialize our variables, batch size, and the begining/end of the batch locs\n",
        "  batch_size = 1500 # number of characters to translate in each iteration\n",
        "  char_index_start = 0  # The start of the index\n",
        "  char_index_end = char_index_start + batch_size # the end of the index\n",
        "  translated_content = ''\n",
        "\n",
        "  # Detect the language of the text\n",
        "  detected_language = translator.detect(content_to_translate[char_index_start:char_index_end])\n",
        "  \n",
        "  # Translate the text in batches and store the text output to 'translated_content'\n",
        "  while char_index_start < len(content_to_translate):\n",
        "      translated_batch = translator.translate(content_to_translate[char_index_start:char_index_end], dest = 'en', src = detected_language.lang)\n",
        "      translated_content = translated_content + translated_batch.text\n",
        "      char_index_start = char_index_end\n",
        "      if char_index_end + batch_size < len(content_to_translate):\n",
        "        char_index_end += batch_size\n",
        "      else: \n",
        "        char_index_end = len(content_to_translate)\n",
        "\n",
        "banner('Translating ' + filename_to_translate)\n",
        "print(translated_content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TpLefWQX-jK",
        "colab_type": "text"
      },
      "source": [
        "## Perform basic sentiment analysis of the translated_content\n",
        "Use TextBlob to find the sentiment of the translated text.  The sentiment property returns a namedtuple of the form Sentiment(polarity, subjectivity). The polarity score is a float within the range [-1.0, 1.0]. The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF5ZcIyHYuRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the TextBlob package\n",
        "from textblob import TextBlob\n",
        "\n",
        "translated_textblob = TextBlob(translated_content)\n",
        "translated_textblob.sentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaQD3__6wVmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "banner('You are a great American!!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_M__dMIiU_k",
        "colab_type": "text"
      },
      "source": [
        "# Export the data and results for later use\n",
        "The documents collected by this code will not persist after the virtual machine shuts down.  However, There are multiple ways to export the zipfile containing all the documents.  Below are two different ways.  I reccomend only running one of these, but if you want to save the file in multiple locations that is fine.\n",
        "\n",
        "1. Zip the files and dataframe with info on the files.\n",
        "2. Export the contents to google drive or download to your computer.  \n",
        "  2.1 Save the content to a user selected folder on googledrive.  \n",
        "  2.2 OR Download the file to the user's local computer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW6egmiD6dL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 1. zip the files and the dataframe with the results\n",
        "pdf_df.to_excel(os.path.join(scrape_folder, 'file_info.xlsx'))\n",
        "%cd {working_folder}\n",
        "export_filename = 'data_export.zip'\n",
        "!zip -r -j {export_filename} {scrape_folder}/*.*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQD8ka8IigdQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 2.1 Mount the users google drive folder to '/gdrive/\n",
        "from google.colab import drive\n",
        "mount_folder = drive.mount('/gdrive')\n",
        "\n",
        "# Select the output folder \n",
        "output_folder = 'scraper_output'\n",
        "\n",
        "output_path = os.path.join(mount_folder, output_folder)\n",
        "os.chdir(working_folder)\n",
        "\n",
        "# Copy the output folder to that location\n",
        "!cp {export_filename} {output_path}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yK00Rw3lz3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 2.2 Download the file to the user's local computer\n",
        "from google.colab import files\n",
        "\n",
        "files.download(export_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF3l0PTHtaML",
        "colab_type": "text"
      },
      "source": [
        "# End of notebook\n",
        "# ===================================\n",
        "Hopefully you saved all the files and learned some cool stuff becuase this is the end of the notebook.  You can factory rest the runtime and then try it on the next website. "
      ]
    }
  ]
}
