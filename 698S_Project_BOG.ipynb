{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "698S-Project-BOG.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PurpleDin0/698S_BOG/blob/master/698S_Project_BOG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAZDZ5_d4gsK",
        "colab_type": "text"
      },
      "source": [
        "# Web Scraper Execution Notebook\n",
        "BLUF: THis notebook imports the relevant python scraping scripts from github, builds the envirnoment, executes the scripts, saves the data locally, performs basic NLP on the recieved data, then prompts the user to save the resulting information/data (either locally, or to Google Drive).\n",
        "\n",
        "* [ ] The websites to be searched are located here [Link to website list](https://drive.google.com/file/d/1y8FL1rBu8yb2HEn7-ok-ILy1Wq3UrDZU/view?usp=sharing).  Note: website list restricted, for access see the author.\n",
        "* [ ] The scrapped data should be searched for the terms included in this file [Link to search terms list](https://drive.google.com/file/d/18MyoEiAYTG6w6JRejv5d4qVK4RxYyRu2/view?usp=sharing).  Note: file access restricted, for access see the author.\n",
        "\n",
        "Problem Statement:\n",
        "Builds a tool to scrape websites and then analyze the information for key terms\n",
        "and provide the analyst with a rank-ordered list of documents that require human review. \n",
        "1. [ ] Construct a set of Python scripts that can be used to scrape a set of [3 websites](https://drive.google.com/file/d/1y8FL1rBu8yb2HEn7-ok-ILy1Wq3UrDZU/view?usp=sharing).  \n",
        "  - [ ] 1.1. Build script to scrape website 1.  \n",
        "  - [ ] 1.2. Build script to scrape website 2.  \n",
        "  - [ ] 1.3. Build script to scrape website 3.  \n",
        "2. [ ] Construct a python Notebook that executes the python script and then performs natural language processing (NLP) to the scrapped data.  \n",
        "  - [ ] 2.1. Build the initial section that executes the python scripts.\n",
        "  - [ ] 2.2. Rank order the scrapped results by relevance to the analyist. The ranking will be based on the [list of key terms](https://drive.google.com/file/d/18MyoEiAYTG6w6JRejv5d4qVK4RxYyRu2/view?usp=sharing).\n",
        "  - [ ] 2.3. include an ability to automate the translation of selected\n",
        "portions of the scraped content.\n",
        "  - [ ] 2.4. Provide the user with the ability to save the scrapped content.\n",
        "\n",
        "- Other possible things: \n",
        "  -  The code should allow the list of keywords to change over time.  \n",
        "  -  The system  should immediately flag certain keywords for human review regardless of other content in the document.  \n",
        "  -  The system should be able to retrieve and analyze office documents (PDFs,\n",
        "spreadsheets, and word processor documents). \n",
        "  - The system should perform sentiment analysis of selected content.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ij9Eho8AizY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/698S_BOG/websites.csv '/content/drive/My Drive/Colab Notebooks/Coursework/698S/Project'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGpNrVuaWeHo",
        "colab_type": "text"
      },
      "source": [
        "# Prepare the environment \n",
        "0. git the repo and install all dependancies\n",
        "1. Install any dependancies\n",
        "2. Load the list of search sites and search terms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6czqjNjv5IN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "## 0. Clone the repo and then change the working directory to the cloned folder\n",
        "git_repo =  '698S_BOG'  # Repo we want to clone\n",
        "git_user = 'PurpleDin0' # User/org we want to clone the repo from\n",
        "!git clone https://github.com/{git_user}/{git_repo}.git\n",
        "repo_dir = os.path.join(os.getcwd(), git_repo)\n",
        "os.chdir(repo_dir)\n",
        "\n",
        "## 1. Install the required libraries\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Caution: Once complete make sure to restart the runtime if required. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsReszSfBH1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 2. Load the list of search sites and search terms\n",
        "\n",
        "# Import PyDrive and associated libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Download a file based on its file ID.\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "# \n",
        "# Get the website list file and load it to a dataframe\n",
        "file_id = '1y8FL1rBu8yb2HEn7-ok-ILy1Wq3UrDZU'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('website_list.csv')\n",
        "website_list_df = pd.read_csv('website_list.csv')\n",
        "# Get the search term file and load it to a dataframe\n",
        "file_id = '18MyoEiAYTG6w6JRejv5d4qVK4RxYyRu2'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('search_terms.csv')\n",
        "search_terms_df = pd.read_csv('search_terms.csv')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjL9glrrmEro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "domain = [s.replace('www.', '') for s in website_list_df.Website.to_list()]\n",
        "website_list_df['Domain'] = domain"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDx9rY5rb7hN",
        "colab_type": "text"
      },
      "source": [
        "#1. Modify the python scripts to target them at the desired websites\n",
        "Todolist:\n",
        " * [ ] edit the python to change the url to a passed variable the targeted site(s) with the desired targets\n",
        " * [ ]  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlHz633Ib5xo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO write a script that removes the generic sites from the imported python code with the desired target(s)\n",
        "#TODO "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb-e4zNwbTPJ",
        "colab_type": "text"
      },
      "source": [
        "#2. Run the spider script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PivlM_MTpkkr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ALLOWED_DOMAINS = [s.replace('www.', '') for s in website_list_df.Website.to_list()]\n",
        "URLS = website_list_df.Website.to_list()\n",
        "print(URLS)\n",
        "print(ALLOWED_DOMAINS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-GrwYIi-m4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cool banner printer\n",
        "def banner(t, s='~'):\n",
        "    l = s * (len(t) + 4)\n",
        "    print(l + '\\n' + '{0} {1} {0}'.format(s, t) + '\\n' + l)\n",
        "\n",
        "# Formatted line\n",
        "def line(t):\n",
        "  print(\"\\n\", \"~\"*(26-(len(t)//2)), t, \"~\"*(26-(len(t)//2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7_T8LmUgZw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "    Following PEP8 Style Guide and Google format function docstrings\n",
        "    Website spider to extract list of URLs and saves to file.\n",
        "    root/\n",
        "    |_____file.json\n",
        "\"\"\"\n",
        "\n",
        "# Standard library imports one per line\n",
        "import sys\n",
        "from urllib.parse import urlparse, urljoin\n",
        "\n",
        "# Third party library imports one per line\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor\n",
        "from scrapy.spiders import Rule\n",
        "\n",
        "# Authorship constants\n",
        "__authors__ = ['BA', 'JP']\n",
        "__copyright__ = \"Copyright 2020, NIU\"\n",
        "__credits__ = [None]\n",
        "__version__ = \"0.1\"\n",
        "__status__ = \"Alpha\"\n",
        "__date__ = \"10 March 2020\"\n",
        "\n",
        "# Global variables\n",
        "visited = list()\n",
        "NAME = 'sixnineeight'\n",
        "ALLOWED_DOMAINS = '' # Base URL goes here\n",
        "URLS = '' # Fully qualified domain goes here\n",
        "\n",
        "\n",
        "class SixnineeightSpider(scrapy.Spider):\n",
        "    \"\"\"\n",
        "        Summary:\n",
        "            `SixnineeightSpider` instantiates the custom spider object.\n",
        "            Parameters:\n",
        "                scrapy.Spider (obj):\n",
        "            Returns:\n",
        "                (None)\n",
        "    \"\"\"\n",
        "    name = NAME\n",
        "    allowed_domains = [ALLOWED_DOMAINS]\n",
        "    start_urls = [URLS]\n",
        "    custom_settings = {\n",
        "        'DEPTH_LIMIT': 1  # Gets deep quick...2 is very deep on large sites e.g. 45K+ unique links on CNN. 0=no limit\n",
        "    }\n",
        "    rules = (\n",
        "        Rule(\n",
        "            LxmlLinkExtractor(allow=()),\n",
        "            callback='parse_obj',\n",
        "            follow=True),\n",
        "    )\n",
        "\n",
        "    def parse(self, response):\n",
        "        \"\"\"\n",
        "            Summary:\n",
        "                `parse` method to parse spider responses.\n",
        "                Parameters:\n",
        "                    self (obj): self\n",
        "                    response (obj): scrapy.Response\n",
        "                Returns:\n",
        "                    (None)\n",
        "        \"\"\"\n",
        "        links = response.xpath('//a//@href').extract()\n",
        "        for link in links:\n",
        "            if \"#\" in link:\n",
        "                link = link.split(\"#\")[0]\n",
        "            if link in visited or urljoin(URLS,\n",
        "                                          link) in visited or \"mailto:\" in link or \"tel:\" in link or \"javascript:\" in link:\n",
        "                continue\n",
        "            else:\n",
        "                if ALLOWED_DOMAINS in link and link in urlparse(link).netloc:\n",
        "                    if urlparse(link).scheme == '':\n",
        "                        link = urljoin(urlparse(URLS).scheme, urlparse(URLS).path, link)\n",
        "                    visited.append(link)\n",
        "                    yield {\n",
        "                        \"link\": link\n",
        "                    }\n",
        "                    continue\n",
        "                if urlparse(link).netloc == '' and link not in visited:\n",
        "                    visited.append(urljoin(URLS, link))\n",
        "                    yield {\n",
        "                        \"link\": urljoin(URLS, link)\n",
        "                    }\n",
        "                    continue\n",
        "                if urlparse(link).netloc != '' and ALLOWED_DOMAINS not in link:\n",
        "                    continue\n",
        "\n",
        "        for next_page in links:\n",
        "            if next_page is not None and urlparse(next_page).netloc == '' or ALLOWED_DOMAINS in next_page:\n",
        "                if urlparse(next_page).netloc == '':\n",
        "                    next_page = urljoin(URLS, next_page)\n",
        "                next_page = response.urljoin(next_page)\n",
        "                yield scrapy.Request(next_page, callback=self.parse, dont_filter=True)\n",
        "\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aPlE8OIg_B9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prints a banner\n",
        "from time import time, ctime, strftime, gmtime\n",
        "\n",
        "def elapsed(start_time):\n",
        "  \"\"\"\n",
        "    Calcuilates the elapsed time of script start of exeution until\n",
        "    code completes. This allows for baselining different runtime enviroments\n",
        "  \"\"\"\n",
        "  current_time = time()\n",
        "  elapsed_time = current_time - start_time\n",
        "  print(\"\\n\\n\" + \"~\"*50)\n",
        "  print(\"             End : \" + ctime(current_time))\n",
        "  print(\"        Run Time : \" + strftime(\"%H:%M:%S\", gmtime(elapsed_time)))\n",
        "  print(\"~\"*50)\n",
        "\n",
        "start_time = time()\n",
        "\n",
        "banner('Running the Spider for links')\n",
        "\n",
        "# Generic spider object with custom settings\n",
        "process = CrawlerProcess(\n",
        "            settings={\n",
        "              # 'FEED_FORMAT': 'pickle',\n",
        "              # 'FEED_URI': 'file:///***/links.pkl',\n",
        "              # 'LOG_LEVEL': 'INFO',  # Uncomment if you don't want scrapy to puke DEBUG to the console\n",
        "              # 'DOWNLOAD_DELAY': 0.25,   # 250 ms of delay, default is random between 0.5 - 1.5\n",
        "              'TELNETCONSOLE_ENABLED': False,  # On by default...that's dumb ¯\\_(ツ)_/¯\n",
        "              'FEED_FORMAT': 'json',\n",
        "              'FEED_URI': 'links.json',\n",
        "              'LOG_LEVEL': 'CRITICAL',\n",
        "              'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',\n",
        "              'CLOSESPIDER_TIMEOUT': 60\n",
        "            } \n",
        "          )\n",
        "\n",
        "# Instantiates the custom spider class object\n",
        "process.crawl(SixnineeightSpider)\n",
        "try:\n",
        "    # Kick off the custom spider and crawl\n",
        "    process.start()\n",
        "\n",
        "    print(\"It worked!!\")\n",
        "    print(\"Output {} file written to {}.\".format(process.settings['FEED_FORMAT'], process.settings['FEED_URI']))\n",
        "    elapsed(start_time)\n",
        "except Exception as e:\n",
        "    print(\"Well, that didn't work... \\n {}\".format(e))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU6cJcqxtqkL",
        "colab_type": "text"
      },
      "source": [
        "# Download the files and create a zip file for processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqTejgOEdYSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from urllib.parse import urlparse, urljoin\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Prints a banner\n",
        "banner('Beginning file downloads')\n",
        "\n",
        "# Sets custom user-agent string to masquerade \n",
        "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36'}\n",
        "\n",
        "# Reads the links fro the json file and sorts for .pdf (or other) filetypes found\n",
        "pdf_links = pd.read_json('/content/698S_BOG/links.json')\n",
        "pdf_links = pdf_links[pdf_links['link'].str.contains(\".pdf\", na=False)] \n",
        "\n",
        "# Loops through the list of links\n",
        "for url in pdf_links.link.values.tolist():\n",
        "  # Gets the basename of the file and if a dynamic link sets a new name\n",
        "  filename = os.path.basename(urlparse(url).path)\n",
        "  if os.path.exists(os.path.join('content', filename)):\n",
        "    filename = filename + urlparse(url).query + '.pdf'\n",
        "  \n",
        "  print('*'*50)\n",
        "  print(\"Requesting: \" + url)\n",
        "  try:\n",
        "    # Checks if content directory exists, if not, create\n",
        "    if not os.path.exists('content'):\n",
        "      os.makedirs('content')\n",
        "\n",
        "    # Makes the request\n",
        "    r = requests.get(url, stream=True, headers=headers)\n",
        "\n",
        "    # Processes if link is valid, or raise generic exception\n",
        "    if r.status_code == 200:\n",
        "      # Chunks the file for progress bar\n",
        "      total_size = int(r.headers.get('content-length', 0))\n",
        "      block_size = 1024 \n",
        "\n",
        "      # Progress bar parameters\n",
        "      t=tqdm(total=total_size, unit='iB', unit_scale=True)\n",
        "      \n",
        "      # Downloads the files to the content folder\n",
        "      with open(os.path.join('content', filename), 'wb') as f:\n",
        "          for data in r.iter_content(block_size):\n",
        "              t.update(len(data))\n",
        "              f.write(data)\n",
        "      \n",
        "      # Error handling\n",
        "      if total_size != 0 and t.n != total_size:\n",
        "        raise\n",
        "      \n",
        "      # Close the progress bar object\n",
        "      t.close()\n",
        "    else:\n",
        "      print('{} Response, Skipping {}.'.format(r.status_code, url))\n",
        "    \n",
        "  except Exception as e:\n",
        "    print(\"Error: {}\".format(e))\n",
        "\n",
        "# Zip up the files\n",
        "banner(\"Zipping up the files\")\n",
        "\n",
        "# Zip object walks directory and packs all files without directory structure\n",
        "try:\n",
        "  ziper = zipfile.ZipFile('content.zip', 'w', zipfile.ZIP_DEFLATED)\n",
        "  for root, dirs, files in os.walk('content/'):\n",
        "    for file in files:\n",
        "      ziper.write(os.path.join(root, file), file)\n",
        "  ziper.close()\n",
        "except Exception as e:\n",
        "  print(\"Could not zip the file. \" + e)\n",
        "\n",
        "# Cleans up the files and removes the content directory\n",
        "try:\n",
        "  if os.path.exists('content'):\n",
        "    for root, dirs, files in os.walk('content/'):\n",
        "      for file in tqdm(files):\n",
        "        os.remove(os.path.join(root, file))\n",
        "    os.removedirs('content')\n",
        "  if os.path.exists('content.zip'):\n",
        "    print('\\nZip file saved')\n",
        "  else:\n",
        "    raise Exception(\"Could not write zip file.\")\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIP2Zs1p1VVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Option one for calling the file\n",
        "#pass the list of sites you want to scrape\n",
        "# %cd {repo_dir}\n",
        "# import sixnineeight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9rQsKhKWoDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Option two for calling the file\n",
        "# %cd /content/698S_BOG\n",
        "# !python3 sixnineeight.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuzvEVZ_VdLm",
        "colab_type": "text"
      },
      "source": [
        "# Parse the documents for the desired text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iox-jtkBcq3n",
        "colab_type": "text"
      },
      "source": [
        "## Import parsing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDgS4r_CdaQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install PyPDF2\n",
        "!pip install tika"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nay_fjEHcp5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import PyPDF2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8jr_PMIc1-X",
        "colab_type": "text"
      },
      "source": [
        "## Load the search times"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSWu-hWXcpJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the search term file and load it to a dataframe\n",
        "file_id = '18MyoEiAYTG6w6JRejv5d4qVK4RxYyRu2'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('search_terms.csv')\n",
        "search_terms_df = pd.read_csv('search_terms.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqFJGQTwO5Iq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# My take on unzipping the files again\n",
        "import zipfile\n",
        "\n",
        "working_folder = os.path.join('/content', '698S_BOG')\n",
        "filename = os.path.join(working_folder, 'content.zip')\n",
        "scrape_folder = os.path.join(working_folder, 'scraped_files')\n",
        "\n",
        "try:\n",
        "  if not os.path.exists(scrape_folder):\n",
        "    os.makedirs(scrape_folder)\n",
        "\n",
        "  with zipfile.ZipFile(filename, 'r') as z:\n",
        "    z.extractall(scrape_folder)\n",
        "    \n",
        "  os.chdir(scrape_folder)\n",
        "  line(\"Files unzipped\")\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kerdc0smVivM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the zip file and unzip\n",
        "# file_id = '1wbud3KFgJ8Su6ChrM87BshB01F7uOm3M'\n",
        "# downloaded = drive.CreateFile({'id': file_id})\n",
        "# filename = 'content.zip' # changed from downloaded.metadata['originalFilename']\n",
        "# downloaded.GetContentFile(filename)\n",
        "# search_terms_df = pd.read_csv('search_terms.csv')\n",
        "# scrape_folder = 'scraped_files'\n",
        "# os.mkdir = scrape_folder\n",
        "\n",
        "# !unzip {filename} -d {scrape_folder}\n",
        "# os.chdir(scrape_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-5et-FpWTsD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# open all the files\n",
        "for file in os.listdir(scrape_folder):\n",
        "    if file.endswith(\".pdf\"):\n",
        "        print(file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtznRV2JgyaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filename = '7980416.pdf'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYl-azCmgSE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "search_terms_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqItI8F4e0dP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "search_terms_df['Term (CN)'].to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11tt5ooSggq6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import tika\n",
        "from tika import parser\n",
        "\n",
        "# Kick off the virtual machine\n",
        "tika.initVM()\n",
        "\n",
        "# Loops through the files\n",
        "banner('Searching the files for terms')\n",
        "\n",
        "for file in os.listdir(scrape_folder):\n",
        "    if file.endswith(\".pdf\"):\n",
        "      parsed = parser.from_file(file)\n",
        "      # Looks for the terms one by one in each file\n",
        "      for term in search_terms_df['Term (CN)'].to_list():\n",
        "        # Action if term found\n",
        "        if term in parsed[\"content\"]:\n",
        "          count = [t.start() for t in re.finditer(term, parsed['content'])] # This is actially a list of the occurence locations, may be useful later ¯\\_(ツ)_/¯\n",
        "          line(\"Found {} occurences of term {} in file {}\".format(len(count), term, file))\n",
        "\n",
        "#print(parsed[\"metadata\"])\n",
        "#print(parsed[\"content\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF3l0PTHtaML",
        "colab_type": "text"
      },
      "source": [
        "## ===================================\n",
        "## !!EVERYTHING WORKES TO THIS POINT!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLxHeNoAcZ2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "pdfFileObj = open(filename,'rb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daQC6-47duiD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pdfFileObj\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWYHCgjJdzVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
        "#Discerning the number of pages will allow us to parse through all the pages.\n",
        "num_pages = pdfReader.numPages\n",
        "count = 0\n",
        "text = \"\"\n",
        "#The while loop will read each page.\n",
        "while count < num_pages:\n",
        "    pageObj = pdfReader.getPage(count)\n",
        "    count +=1\n",
        "    text += pageObj.extractText()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6JJOXOzeHZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vVWOOZheIH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pdfReader.getDocumentInfo()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEnSxHNRfFtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "page = pdfReader.getPage(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLsPNGohfZN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "page.getContents()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL3ZhaIjflmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}