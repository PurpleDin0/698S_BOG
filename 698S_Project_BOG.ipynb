{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "698S-Project-BOG.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "7-5zfYftLLdP",
        "YF3l0PTHtaML",
        "bOaYwt1_Oveb",
        "94g9T_L1Ju85"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PurpleDin0/698S_BOG/blob/master/698S_Project_BOG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAZDZ5_d4gsK",
        "colab_type": "text"
      },
      "source": [
        "# Web Scraper Execution Notebook\n",
        "BLUF: THis notebook imports the relevant python scraping scripts from github, builds the envirnoment, executes the scripts, saves the data locally, performs basic NLP on the recieved data, then prompts the user to save the resulting information/data (either locally, or to Google Drive).\n",
        "\n",
        "* [ ] The websites to be searched are located here [Link to website list](https://drive.google.com/file/d/1y8FL1rBu8yb2HEn7-ok-ILy1Wq3UrDZU/view?usp=sharing).  Note: website list restricted, for access see the author.\n",
        "* [ ] The scrapped data should be searched for the terms included in this file [Link to search terms list](https://drive.google.com/file/d/18MyoEiAYTG6w6JRejv5d4qVK4RxYyRu2/view?usp=sharing).  Note: file access restricted, for access see the author.\n",
        "\n",
        "Problem Statement:\n",
        "Builds a tool to scrape websites and then analyze the information for key terms\n",
        "and provide the analyst with a rank-ordered list of documents that require human review. \n",
        "1. [ ] Construct a set of Python scripts that can be used to scrape a set of [3 websites](https://drive.google.com/file/d/1y8FL1rBu8yb2HEn7-ok-ILy1Wq3UrDZU/view?usp=sharing).  \n",
        "  - [ ] 1.1. Build script to scrape website 1.  \n",
        "  - [ ] 1.2. Build script to scrape website 2.  \n",
        "  - [ ] 1.3. Build script to scrape website 3.  \n",
        "2. [ ] Construct a python Notebook that executes the python script and then performs natural language processing (NLP) to the scrapped data.  \n",
        "  - [ ] 2.1. Build the initial section that executes the python scripts.\n",
        "  - [ ] 2.2. Rank order the scrapped results by relevance to the analyist. The ranking will be based on the [list of key terms](https://drive.google.com/file/d/18MyoEiAYTG6w6JRejv5d4qVK4RxYyRu2/view?usp=sharing).\n",
        "  - [ ] 2.3. include an ability to automate the translation of selected\n",
        "portions of the scraped content.\n",
        "  - [ ] 2.4. Provide the user with the ability to save the scrapped content.\n",
        "\n",
        "- Other possible things: \n",
        "  -  The code should allow the list of keywords to change over time.  \n",
        "  -  The system  should immediately flag certain keywords for human review regardless of other content in the document.  \n",
        "  -  The system should be able to retrieve and analyze office documents (PDFs,\n",
        "spreadsheets, and word processor documents). \n",
        "  - The system should perform sentiment analysis of selected content.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ij9Eho8AizY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/698S_BOG/websites.csv '/content/drive/My Drive/Colab Notebooks/Coursework/698S/Project'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGpNrVuaWeHo",
        "colab_type": "text"
      },
      "source": [
        "# Prepare the environment \n",
        "0. git the repo and install all dependancies\n",
        "1. Install any dependancies\n",
        "2. Load the list of search sites and search terms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6czqjNjv5IN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "827cd073-6918-4636-ee2b-0d810420c9ae"
      },
      "source": [
        "import os\n",
        "## 0. Clone the repo and then change the working directory to the cloned folder\n",
        "git_repo =  '698S_BOG'  # Repo we want to clone\n",
        "git_user = 'PurpleDin0' # User/org we want to clone the repo from\n",
        "!git clone https://github.com/{git_user}/{git_repo}.git\n",
        "repo_dir = os.path.join(os.getcwd(), git_repo)\n",
        "os.chdir(repo_dir)\n",
        "\n",
        "## 1. Install the required libraries\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Caution: Once complete make sure to restart the runtime if required. "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '698S_BOG'...\n",
            "remote: Enumerating objects: 69, done.\u001b[K\n",
            "remote: Counting objects: 100% (69/69), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 69 (delta 35), reused 18 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (69/69), done.\n",
            "Collecting Scrapy==1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/45/414e87ac8209d537c91575538c5307c20217a6943f555e0ee39f6db4bb0f/Scrapy-1.6.0-py2.py3-none-any.whl (231kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 9.3MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/b7/34eec2fe5a49718944e215fde81288eec1fa04638aa3fb57c1c6cd0f98c3/beautifulsoup4-4.8.0-py3-none-any.whl (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.0MB/s \n",
            "\u001b[?25hCollecting requests==2.22.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.8MB/s \n",
            "\u001b[?25hCollecting tqdm==4.36.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/c1/bc1dba38b48f4ae3c4428aea669c5e27bd5a7642a74c8348451e0bd8ff86/tqdm-4.36.1-py2.py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.6MB/s \n",
            "\u001b[?25hCollecting pyOpenSSL\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/de/f8342b68fa9e981d348039954657bdf681b2ab93de27443be51865ffa310/pyOpenSSL-19.1.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.3MB/s \n",
            "\u001b[?25hCollecting parsel>=1.5\n",
            "  Downloading https://files.pythonhosted.org/packages/23/1e/9b39d64cbab79d4362cdd7be7f5e9623d45c4a53b3f7522cd8210df52d8e/parsel-1.6.0-py2.py3-none-any.whl\n",
            "Collecting Twisted>=13.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/04/1a664c9e5ec0224a1c1a154ddecaa4dc7b8967521bba225efcc41a03d5f3/Twisted-20.3.0-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 31.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from Scrapy==1.6.0->-r requirements.txt (line 1)) (4.2.6)\n",
            "Collecting queuelib\n",
            "  Downloading https://files.pythonhosted.org/packages/4c/85/ae64e9145f39dd6d14f8af3fa809a270ef3729f3b90b3c0cf5aa242ab0d4/queuelib-1.5.0-py2.py3-none-any.whl\n",
            "Collecting cssselect>=0.9\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Collecting PyDispatcher>=2.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/37/39aca520918ce1935bea9c356bcbb7ed7e52ad4e31bff9b943dfc8e7115b/PyDispatcher-2.0.5.tar.gz\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from Scrapy==1.6.0->-r requirements.txt (line 1)) (1.12.0)\n",
            "Collecting w3lib>=1.17.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/59/b6b14521090e7f42669cafdb84b0ab89301a42f1f1a82fcf5856661ea3a7/w3lib-1.22.0-py2.py3-none-any.whl\n",
            "Collecting service-identity\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/7c/2195b890023e098f9618d43ebc337d83c8b38d414326685339eb024db2f6/service_identity-18.1.0-py2.py3-none-any.whl\n",
            "Collecting soupsieve>=1.2\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/8f/457f4a5390eeae1cc3aeab89deb7724c965be841ffca6cfca9197482e470/soupsieve-2.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Collecting idna<2.9,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->-r requirements.txt (line 3)) (2020.4.5.2)\n",
            "Collecting cryptography>=2.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/04/686efee2dcdd25aecf357992e7d9362f443eb182ecd623f882bc9f7a6bba/cryptography-2.9.2-cp35-abi3-manylinux2010_x86_64.whl (2.7MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 72.3MB/s \n",
            "\u001b[?25hCollecting PyHamcrest!=1.10.0,>=1.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/16/e54cc65891f01cb62893540f44ffd3e8dab0a22443e1b438f1a9f5574bee/PyHamcrest-2.0.2-py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.3MB/s \n",
            "\u001b[?25hCollecting incremental>=16.10.1\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/1d/c98a587dc06e107115cf4a58b49de20b19222c83d75335a192052af4c4b7/incremental-17.5.0-py2.py3-none-any.whl\n",
            "Collecting constantly>=15.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b9/65/48c1909d0c0aeae6c10213340ce682db01b48ea900a7d9fce7a7910ff318/constantly-15.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=13.1.0->Scrapy==1.6.0->-r requirements.txt (line 1)) (19.3.0)\n",
            "Collecting Automat>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/dd/83/5f6f3c1a562674d65efc320257bdc0873ec53147835aeef7762fe7585273/Automat-20.2.0-py2.py3-none-any.whl\n",
            "Collecting hyperlink>=17.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/7f/91/e916ca10a2de1cb7101a9b24da546fb90ee14629e23160086cf3361c4fb8/hyperlink-19.0.0-py2.py3-none-any.whl\n",
            "Collecting zope.interface>=4.4.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/33/565274c28a11af60b7cfc0519d46bde4125fcd7d32ebc0a81b480d0e8da6/zope.interface-5.1.0-cp36-cp36m-manylinux2010_x86_64.whl (234kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 64.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1-modules in /usr/local/lib/python3.6/dist-packages (from service-identity->Scrapy==1.6.0->-r requirements.txt (line 1)) (0.2.8)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.6/dist-packages (from service-identity->Scrapy==1.6.0->-r requirements.txt (line 1)) (0.4.8)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.8->pyOpenSSL->Scrapy==1.6.0->-r requirements.txt (line 1)) (1.14.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from zope.interface>=4.4.2->Twisted>=13.1.0->Scrapy==1.6.0->-r requirements.txt (line 1)) (47.3.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.8->pyOpenSSL->Scrapy==1.6.0->-r requirements.txt (line 1)) (2.20)\n",
            "Building wheels for collected packages: PyDispatcher\n",
            "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-cp36-none-any.whl size=11515 sha256=26ebe20ee9d6492aefa420a9353ff6e9c94fc0a792db57bd67768ba2c185ee8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/99/96/cfef6665f9cb1522ee6757ae5955feedf2fe25f1737f91fa7f\n",
            "Successfully built PyDispatcher\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement tqdm<5.0.0,>=4.38.0, but you'll have tqdm 4.36.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: cryptography, pyOpenSSL, w3lib, cssselect, parsel, PyHamcrest, incremental, constantly, Automat, idna, hyperlink, zope.interface, Twisted, queuelib, PyDispatcher, service-identity, Scrapy, soupsieve, beautifulsoup4, requests, tqdm\n",
            "  Found existing installation: idna 2.9\n",
            "    Uninstalling idna-2.9:\n",
            "      Successfully uninstalled idna-2.9\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 PyHamcrest-2.0.2 Scrapy-1.6.0 Twisted-20.3.0 beautifulsoup4-4.8.0 constantly-15.1.0 cryptography-2.9.2 cssselect-1.1.0 hyperlink-19.0.0 idna-2.8 incremental-17.5.0 parsel-1.6.0 pyOpenSSL-19.1.0 queuelib-1.5.0 requests-2.22.0 service-identity-18.1.0 soupsieve-2.0.1 tqdm-4.36.1 w3lib-1.22.0 zope.interface-5.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "idna",
                  "requests",
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsReszSfBH1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 2. Load the list of search sites and search terms\n",
        "\n",
        "# Import PyDrive and associated libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Download a file based on its file ID.\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "# \n",
        "# Get the website list file and load it to a dataframe\n",
        "file_id = '1y8FL1rBu8yb2HEn7-ok-ILy1Wq3UrDZU'  ## USER UPDATEABLE VALUE! ##\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.FetchMetadata()\n",
        "filename = downloaded.metadata['originalFilename']\n",
        "downloaded.GetContentFile(filename)\n",
        "website_list_df = pd.read_csv(filename)\n",
        "# Get the search term file and load it to a dataframe\n",
        "file_id = '18MyoEiAYTG6w6JRejv5d4qVK4RxYyRu2'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.FetchMetadata()\n",
        "filename = downloaded.metadata['originalFilename'] ## USER UPDATEABLE VALUE! ##\n",
        "downloaded.GetContentFile(filename)\n",
        "search_terms_df = pd.read_csv(filename)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjL9glrrmEro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "domain = [s.replace('www.', '') for s in website_list_df.Website.to_list()]\n",
        "website_list_df['Domain'] = domain"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-GrwYIi-m4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cool banner printer\n",
        "def banner(t, s='~'):\n",
        "    l = s * (len(t) + 4)\n",
        "    print(l + '\\n' + '{0} {1} {0}'.format(s, t) + '\\n' + l)\n",
        "\n",
        "# Formatted line\n",
        "def line(t):\n",
        "  print(\"\\n\", \"~\"*(26-(len(t)//2)), t, \"~\"*(26-(len(t)//2)))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDx9rY5rb7hN",
        "colab_type": "text"
      },
      "source": [
        "#1. Modify the python scripts to target them at the desired websites\n",
        "Todolist:\n",
        " * [ ] edit the python to change the url to a passed variable the targeted site(s) with the desired targets\n",
        " * [ ]  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlHz633Ib5xo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO write a script that removes the generic sites from the imported python code with the desired target(s)\n",
        "#TODO "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb-e4zNwbTPJ",
        "colab_type": "text"
      },
      "source": [
        "#2. Run the spider script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PivlM_MTpkkr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d9c58d03-3184-4dd2-d965-55952e76dd39"
      },
      "source": [
        "ALLOWED_DOMAINS = [s.replace('www.', '') for s in website_list_df.Website.to_list()]\n",
        "URLS = website_list_df.Website.to_list()\n",
        "print(URLS)\n",
        "print(ALLOWED_DOMAINS)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['www.miit.gov.cn', 'www.sastind.gov.cn', 'www.jmjh.miit.gov.cn']\n",
            "['miit.gov.cn', 'sastind.gov.cn', 'jmjh.miit.gov.cn']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fvkh65rjQR48",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "26100d8e-cd52-4cad-a075-e9786f033305"
      },
      "source": [
        "!python3 sixnineeight.py ALLOWED_DOMAINS='miit.gov.cn' URLS='http://www.miit.gov.cn'"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "~ Running the Spider for links ~\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "It worked!!\n",
            "Output json file written to links.json.\n",
            "\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "             End : Mon Jun 22 00:49:59 2020\n",
            "        Run Time : 00:03:06\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-5zfYftLLdP",
        "colab_type": "text"
      },
      "source": [
        "## Load the script into memory\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7_T8LmUgZw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "    Following PEP8 Style Guide and Google format function docstrings\n",
        "    Website spider to extract list of URLs and saves to file.\n",
        "    root/\n",
        "    |_____file.json\n",
        "\"\"\"\n",
        "\n",
        "# Standard library imports one per line\n",
        "import sys\n",
        "from urllib.parse import urlparse, urljoin\n",
        "\n",
        "# Third party library imports one per line\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor\n",
        "from scrapy.spiders import Rule\n",
        "\n",
        "# Authorship constants\n",
        "__authors__ = ['BA', 'JP']\n",
        "__copyright__ = \"Copyright 2020, NIU\"\n",
        "__credits__ = [None]\n",
        "__version__ = \"0.1\"\n",
        "__status__ = \"Alpha\"\n",
        "__date__ = \"10 March 2020\"\n",
        "\n",
        "# Global variables\n",
        "visited = list()\n",
        "NAME = 'sixnineeight'\n",
        "ALLOWED_DOMAINS = '' # Base URL goes here  ## USER UPDATEABLE VALUE! ##\n",
        "URLS = '' # Fully qualified domain goes here  ## USER UPDATEABLE VALUE! ##\n",
        "\n",
        "\n",
        "class SixnineeightSpider(scrapy.Spider):\n",
        "    \"\"\"\n",
        "        Summary:\n",
        "            `SixnineeightSpider` instantiates the custom spider object.\n",
        "            Parameters:\n",
        "                scrapy.Spider (obj):\n",
        "            Returns:\n",
        "                (None)\n",
        "    \"\"\"\n",
        "    name = NAME\n",
        "    allowed_domains = [ALLOWED_DOMAINS]\n",
        "    start_urls = [URLS]\n",
        "    custom_settings = {\n",
        "        'DEPTH_LIMIT': 1  # Gets deep quick...2 is very deep on large sites e.g. 45K+ unique links on CNN. 0=no limit\n",
        "    }\n",
        "    rules = (\n",
        "        Rule(\n",
        "            LxmlLinkExtractor(allow=()),\n",
        "            callback='parse_obj',\n",
        "            follow=True),\n",
        "    )\n",
        "\n",
        "    def parse(self, response):\n",
        "        \"\"\"\n",
        "            Summary:\n",
        "                `parse` method to parse spider responses.\n",
        "                Parameters:\n",
        "                    self (obj): self\n",
        "                    response (obj): scrapy.Response\n",
        "                Returns:\n",
        "                    (None)\n",
        "        \"\"\"\n",
        "        links = response.xpath('//a//@href').extract()\n",
        "        for link in links:\n",
        "            if \"#\" in link:\n",
        "                link = link.split(\"#\")[0]\n",
        "            if link in visited or urljoin(URLS,\n",
        "                                          link) in visited or \"mailto:\" in link or \"tel:\" in link or \"javascript:\" in link:\n",
        "                continue\n",
        "            else:\n",
        "                if ALLOWED_DOMAINS in link and link in urlparse(link).netloc:\n",
        "                    if urlparse(link).scheme == '':\n",
        "                        link = urljoin(urlparse(URLS).scheme, urlparse(URLS).path, link)\n",
        "                    visited.append(link)\n",
        "                    yield {\n",
        "                        \"link\": link\n",
        "                    }\n",
        "                    continue\n",
        "                if urlparse(link).netloc == '' and link not in visited:\n",
        "                    visited.append(urljoin(URLS, link))\n",
        "                    yield {\n",
        "                        \"link\": urljoin(URLS, link)\n",
        "                    }\n",
        "                    continue\n",
        "                if urlparse(link).netloc != '' and ALLOWED_DOMAINS not in link:\n",
        "                    continue\n",
        "\n",
        "        for next_page in links:\n",
        "            if next_page is not None and urlparse(next_page).netloc == '' or ALLOWED_DOMAINS in next_page:\n",
        "                if urlparse(next_page).netloc == '':\n",
        "                    next_page = urljoin(URLS, next_page)\n",
        "                next_page = response.urljoin(next_page)\n",
        "                yield scrapy.Request(next_page, callback=self.parse, dont_filter=True)\n",
        "\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aPlE8OIg_B9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prints a banner\n",
        "from time import time, ctime, strftime, gmtime\n",
        "\n",
        "def elapsed(start_time):\n",
        "  \"\"\"\n",
        "    Calcuilates the elapsed time of script start of exeution until\n",
        "    code completes. This allows for baselining different runtime enviroments\n",
        "  \"\"\"\n",
        "  current_time = time()\n",
        "  elapsed_time = current_time - start_time\n",
        "  print(\"\\n\\n\" + \"~\"*50)\n",
        "  print(\"             End : \" + ctime(current_time))\n",
        "  print(\"        Run Time : \" + strftime(\"%H:%M:%S\", gmtime(elapsed_time)))\n",
        "  print(\"~\"*50)\n",
        "\n",
        "start_time = time()\n",
        "\n",
        "banner('Running the Spider for links')\n",
        "\n",
        "# Generic spider object with custom settings\n",
        "process = CrawlerProcess(\n",
        "            settings={\n",
        "              # 'FEED_FORMAT': 'pickle',\n",
        "              # 'FEED_URI': 'file:///***/links.pkl',\n",
        "              # 'LOG_LEVEL': 'INFO',  # Uncomment if you don't want scrapy to puke DEBUG to the console\n",
        "              # 'DOWNLOAD_DELAY': 0.25,   # 250 ms of delay, default is random between 0.5 - 1.5\n",
        "              'TELNETCONSOLE_ENABLED': False,  # On by default...that's dumb ¯\\_(ツ)_/¯\n",
        "              'FEED_FORMAT': 'json',\n",
        "              'FEED_URI': 'links.json',\n",
        "              'LOG_LEVEL': 'CRITICAL',\n",
        "              'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',\n",
        "              'CLOSESPIDER_TIMEOUT': 60\n",
        "            } \n",
        "          )\n",
        "\n",
        "# Instantiates the custom spider class object\n",
        "process.crawl(SixnineeightSpider)\n",
        "try:\n",
        "    # Kick off the custom spider and crawl\n",
        "    process.start()\n",
        "\n",
        "    print(\"It worked!!\")\n",
        "    print(\"Output {} file written to {}.\".format(process.settings['FEED_FORMAT'], process.settings['FEED_URI']))\n",
        "    elapsed(start_time)\n",
        "except Exception as e:\n",
        "    print(\"Well, that didn't work... \\n {}\".format(e))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQM5OB3QLegR",
        "colab_type": "text"
      },
      "source": [
        "## Execute the script that gets the links\n",
        "\n",
        "- [ ] run this to execute the "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU6cJcqxtqkL",
        "colab_type": "text"
      },
      "source": [
        "# Download the files and create a zip file for processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqTejgOEdYSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from urllib.parse import urlparse, urljoin\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Prints a banner\n",
        "banner('Beginning file downloads')\n",
        "\n",
        "# Sets custom user-agent string to masquerade \n",
        "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36'}\n",
        "\n",
        "# Reads the links fro the json file and sorts for .pdf (or other) filetypes found\n",
        "pdf_links = pd.read_json('/content/698S_BOG/links.json')\n",
        "pdf_links = pdf_links[pdf_links['link'].str.contains(\".pdf\", na=False)] \n",
        "\n",
        "# Loops through the list of links\n",
        "for url in pdf_links.link.values.tolist():\n",
        "  # Gets the basename of the file and if a dynamic link sets a new name\n",
        "  filename = os.path.basename(urlparse(url).path)\n",
        "  if os.path.exists(os.path.join('content', filename)):\n",
        "    filename = filename + urlparse(url).query + '.pdf'\n",
        "  \n",
        "  print('*'*50)\n",
        "  print(\"Requesting: \" + url)\n",
        "  try:\n",
        "    # Checks if content directory exists, if not, create\n",
        "    if not os.path.exists('content'):\n",
        "      os.makedirs('content')\n",
        "\n",
        "    # Makes the request\n",
        "    r = requests.get(url, stream=True, headers=headers)\n",
        "\n",
        "    # Processes if link is valid, or raise generic exception\n",
        "    if r.status_code == 200:\n",
        "      # Chunks the file for progress bar\n",
        "      total_size = int(r.headers.get('content-length', 0))\n",
        "      block_size = 1024 \n",
        "\n",
        "      # Progress bar parameters\n",
        "      t=tqdm(total=total_size, unit='iB', unit_scale=True)\n",
        "      \n",
        "      # Downloads the files to the content folder\n",
        "      with open(os.path.join('content', filename), 'wb') as f:\n",
        "          for data in r.iter_content(block_size):\n",
        "              t.update(len(data))\n",
        "              f.write(data)\n",
        "      \n",
        "      # Error handling\n",
        "      if total_size != 0 and t.n != total_size:\n",
        "        raise\n",
        "      \n",
        "      # Close the progress bar object\n",
        "      t.close()\n",
        "    else:\n",
        "      print('{} Response, Skipping {}.'.format(r.status_code, url))\n",
        "    \n",
        "  except Exception as e:\n",
        "    print(\"Error: {}\".format(e))\n",
        "\n",
        "# Zip up the files\n",
        "banner(\"Zipping up the files\")\n",
        "\n",
        "# Zip object walks directory and packs all files without directory structure\n",
        "try:\n",
        "  ziper = zipfile.ZipFile('content.zip', 'w', zipfile.ZIP_DEFLATED)\n",
        "  for root, dirs, files in os.walk('content/'):\n",
        "    for file in files:\n",
        "      ziper.write(os.path.join(root, file), file)\n",
        "  ziper.close()\n",
        "except Exception as e:\n",
        "  print(\"Could not zip the file. \" + e)\n",
        "\n",
        "# Cleans up the files and removes the content directory\n",
        "try:\n",
        "  if os.path.exists('content'):\n",
        "    for root, dirs, files in os.walk('content/'):\n",
        "      for file in tqdm(files):\n",
        "        os.remove(os.path.join(root, file))\n",
        "    os.removedirs('content')\n",
        "  if os.path.exists('content.zip'):\n",
        "    print('\\nZip file saved')\n",
        "  else:\n",
        "    raise Exception(\"Could not write zip file.\")\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIP2Zs1p1VVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Option one for calling the file\n",
        "#pass the list of sites you want to scrape\n",
        "# %cd {repo_dir}\n",
        "# import sixnineeight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9rQsKhKWoDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Option two for calling the file\n",
        "# %cd /content/698S_BOG\n",
        "# !python3 sixnineeight.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuzvEVZ_VdLm",
        "colab_type": "text"
      },
      "source": [
        "# Parse the documents for the desired text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iox-jtkBcq3n",
        "colab_type": "text"
      },
      "source": [
        "## Import parsing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDgS4r_CdaQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tika"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nay_fjEHcp5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import packages needed form parsing\n",
        "import os\n",
        "import pandas as pd\n",
        "import tika\n",
        "tika.initVM()\n",
        "from tika import parser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8jr_PMIc1-X",
        "colab_type": "text"
      },
      "source": [
        "## Load the search times"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqFJGQTwO5Iq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# My take on unzipping the files again\n",
        "import zipfile\n",
        "\n",
        "working_folder = os.path.join('/content', '698S_BOG')  ## DEBUG ## hard_coded\n",
        "filename = os.path.join(working_folder, 'content.zip')\n",
        "scrape_folder = os.path.join(working_folder, 'scraped_files')\n",
        "\n",
        "try:\n",
        "  if not os.path.exists(scrape_folder):\n",
        "    os.makedirs(scrape_folder)\n",
        "\n",
        "  with zipfile.ZipFile(filename, 'r') as z:\n",
        "    z.extractall(scrape_folder)\n",
        "    \n",
        "  os.chdir(scrape_folder)\n",
        "  line(\"Files unzipped\")\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSWu-hWXcpJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the search term file and load it to a dataframe\n",
        "file_id = '18MyoEiAYTG6w6JRejv5d4qVK4RxYyRu2'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.FetchMetadata()\n",
        "filename = downloaded.metadata['originalFilename']\n",
        "downloaded.GetContentFile(filename)\n",
        "search_terms_df = pd.read_csv(filename)\n",
        "\n",
        "os.chdir(scrape_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLj4Mv-DKPgH",
        "colab_type": "text"
      },
      "source": [
        "## Open the PDFs and parse the contents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWYHCgjJdzVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# open all the PDFs and parse their content into dataframes\n",
        "pdf_df = pd.DataFrame()\n",
        "for filename in os.listdir(scrape_folder):\n",
        "    if filename.endswith(\".pdf\"):\n",
        "        parsed = parser.from_file(filename)\n",
        "        row = pd.DataFrame(\n",
        "            [[filename, scrape_folder, parsed[\"metadata\"], parsed[\"content\"]]],\n",
        "            index = [filename],\n",
        "            columns = ['filename', 'folder', 'metadata', 'content'])\n",
        "        pdf_df = pdf_df.append(row)\n",
        "for term in search_terms_df['Term (CN)']:\n",
        "    pdf_df[term] = pdf_df['content'].apply(lambda x : x.count(term))\n",
        "\n",
        "\n",
        "################## future CODE ###############\n",
        "#def content_score([occurance_list]):\n",
        "#  for term in search_terms_df['Term (CN)']:\n",
        "    \n",
        "#for \n",
        "#pdf_df['sum'] = pdf_df[search_terms_df['Term (CN)']].apply(lambda x : x)\n",
        "#pdf_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbIdhfXnO430",
        "colab_type": "text"
      },
      "source": [
        "### Show the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeswSbWHO4Rp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pdf_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF3l0PTHtaML",
        "colab_type": "text"
      },
      "source": [
        "# ===================================\n",
        "# !!EVERYTHING WORKES TO THIS POINT!!\n",
        "\n",
        "Everything down here is trash (and @PurpleDin0's)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOaYwt1_Oveb",
        "colab_type": "text"
      },
      "source": [
        "## OLD CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kerdc0smVivM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the zip file and unzip\n",
        "# file_id = '1wbud3KFgJ8Su6ChrM87BshB01F7uOm3M'\n",
        "# downloaded = drive.CreateFile({'id': file_id})\n",
        "# filename = 'content.zip' # changed from downloaded.metadata['originalFilename']\n",
        "# downloaded.GetContentFile(filename)\n",
        "# search_terms_df = pd.read_csv('search_terms.csv')\n",
        "# scrape_folder = 'scraped_files'\n",
        "# os.mkdir = scrape_folder\n",
        "\n",
        "# !unzip {filename} -d {scrape_folder}\n",
        "# os.chdir(scrape_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-5et-FpWTsD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# open all the files\n",
        "for file in os.listdir(scrape_folder):\n",
        "    if file.endswith(\".pdf\"):\n",
        "        print(file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtznRV2JgyaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filename = '7980416.pdf'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYl-azCmgSE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "search_terms_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqItI8F4e0dP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "search_terms_df['Term (CN)'].to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11tt5ooSggq6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import tika\n",
        "from tika import parser\n",
        "\n",
        "# Kick off the virtual machine\n",
        "tika.initVM()\n",
        "\n",
        "# Loops through the files\n",
        "banner('Searching the files for terms')\n",
        "\n",
        "for file in os.listdir(scrape_folder):\n",
        "    if file.endswith(\".pdf\"):\n",
        "      parsed = parser.from_file(file)\n",
        "      # Looks for the terms one by one in each file\n",
        "      for term in search_terms_df['Term (CN)'].to_list():\n",
        "        # Action if term found\n",
        "        if term in parsed[\"content\"]:\n",
        "          count = [t.start() for t in re.finditer(term, parsed['content'])] # This is actially a list of the occurence locations, may be useful later ¯\\_(ツ)_/¯\n",
        "          line(\"Found {} occurences of term {} in file {}\".format(len(count), term, file))\n",
        "\n",
        "#print(parsed[\"metadata\"])\n",
        "#print(parsed[\"content\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94g9T_L1Ju85",
        "colab_type": "text"
      },
      "source": [
        "## Parse the documents for the desired text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIxHCvyOJxqA",
        "colab_type": "text"
      },
      "source": [
        "## Import parsing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLxHeNoAcZ2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tika\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daQC6-47duiD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import packages needed form parsing\n",
        "import os\n",
        "import pandas as pdimport tika\n",
        "tika.initVM()\n",
        "from tika import parser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNuQh4U7KIK9",
        "colab_type": "text"
      },
      "source": [
        "## Load the search terms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLzvkZv-KK8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the search term file and load it to a dataframe\n",
        "file_id = '18MyoEiAYTG6w6JRejv5d4qVK4RxYyRu2'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.FetchMetadata()\n",
        "filename = downloaded.metadata['originalFilename']\n",
        "downloaded.GetContentFile(filename)\n",
        "search_terms_df = pd.read_csv(filename)\n",
        "\n",
        "os.chdir(scrape_folder)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}