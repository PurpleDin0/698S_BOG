{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "698S-Project-BOG.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PurpleDin0/698S_BOG/blob/master/698S_Project_BOG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAZDZ5_d4gsK",
        "colab_type": "text"
      },
      "source": [
        "# Web Scraper Execution Notebook\n",
        "BLUF: THis notebook imports the relevant python scraping scripts from github, builds the envirnoment, executes the scripts, saves the data locally, performs basic NLP on the recieved data, then prompts the user to save the resulting information/data (either locally, or to Google Drive).\n",
        "\n",
        "* [ ] The websites to be searched are located here [Link to website list](https://drive.google.com/file/d/1y8FL1rBu8yb2HEn7-ok-ILy1Wq3UrDZU/view?usp=sharing).  Note: website list restricted, for access see the author.\n",
        "* [ ] The scrapped data should be searched for the terms included in this file [Link to search terms list](https://drive.google.com/file/d/18MyoEiAYTG6w6JRejv5d4qVK4RxYyRu2/view?usp=sharing).  Note: file access restricted, for access see the author.\n",
        "\n",
        "Problem Statement:\n",
        "Builds a tool to scrape websites and then analyze the information for key terms\n",
        "and provide the analyst with a rank-ordered list of documents that require human review. \n",
        "1. [ ] Construct a set of Python scripts that can be used to scrape a set of [3 websites](https://drive.google.com/file/d/1y8FL1rBu8yb2HEn7-ok-ILy1Wq3UrDZU/view?usp=sharing).  \n",
        "  - [ ] 1.1. Build script to scrape website 1.  \n",
        "  - [ ] 1.2. Build script to scrape website 2.  \n",
        "  - [ ] 1.3. Build script to scrape website 3.  \n",
        "2. [ ] Construct a python Notebook that executes the python script and then performs natural language processing (NLP) to the scrapped data.  \n",
        "  - [ ] 2.1. Build the initial section that executes the python scripts.\n",
        "  - [ ] 2.2. Rank order the scrapped results by relevance to the analyist. The ranking will be based on the [list of key terms](https://drive.google.com/file/d/18MyoEiAYTG6w6JRejv5d4qVK4RxYyRu2/view?usp=sharing).\n",
        "  - [ ] 2.3. include an ability to automate the translation of selected\n",
        "portions of the scraped content.\n",
        "  - [ ] 2.4. Provide the user with the ability to save the scrapped content.\n",
        "\n",
        "- Other possible things: \n",
        "  -  The code should allow the list of keywords to change over time.  \n",
        "  -  The system  should immediately flag certain keywords for human review regardless of other content in the document.  \n",
        "  -  The system should be able to retrieve and analyze office documents (PDFs,\n",
        "spreadsheets, and word processor documents). \n",
        "  - The system should perform sentiment analysis of selected content.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ij9Eho8AizY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "30100447-7488-44b3-821e-6fac7565fd97"
      },
      "source": [
        "!cp /content/698S_BOG/websites.csv '/content/drive/My Drive/Colab Notebooks/Coursework/698S/Project'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot stat '/content/698S_BOG/websites.csv': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGpNrVuaWeHo",
        "colab_type": "text"
      },
      "source": [
        "# Prepare the environment \n",
        "0. git the repo and install all dependancies\n",
        "1. Install any dependancies\n",
        "2. Load the list of search sites and search terms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6czqjNjv5IN",
        "colab_type": "code",
        "outputId": "6b691440-5950-4ecf-c466-9642d805c9ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "import os\n",
        "## 0. Clone the repo and then change the working directory to the cloned folder\n",
        "git_repo =  '698S_BOG'  # Repo we want to clone\n",
        "git_user = 'PurpleDin0' # User/org we want to clone the repo from\n",
        "!git clone https://github.com/{git_user}/{git_repo}.git\n",
        "repo_dir = os.path.join(os.getcwd(), git_repo)\n",
        "%cd {repo_dir}\n",
        "\n",
        "## 1. Install the required libraries\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Caution: Once complete make sure to restart the runtime if required. "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path '698S_BOG' already exists and is not an empty directory.\n",
            "/content/698S_BOG\n",
            "Requirement already satisfied: Scrapy==1.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.6.0)\n",
            "Requirement already satisfied: beautifulsoup4==4.8.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (4.8.0)\n",
            "Requirement already satisfied: requests==2.22.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.22.0)\n",
            "Requirement already satisfied: tqdm==4.36.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (4.36.1)\n",
            "Requirement already satisfied: queuelib in /usr/local/lib/python3.6/dist-packages (from Scrapy==1.6.0->-r requirements.txt (line 1)) (1.5.0)\n",
            "Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.6/dist-packages (from Scrapy==1.6.0->-r requirements.txt (line 1)) (1.22.0)\n",
            "Requirement already satisfied: service-identity in /usr/local/lib/python3.6/dist-packages (from Scrapy==1.6.0->-r requirements.txt (line 1)) (18.1.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from Scrapy==1.6.0->-r requirements.txt (line 1)) (4.2.6)\n",
            "Requirement already satisfied: parsel>=1.5 in /usr/local/lib/python3.6/dist-packages (from Scrapy==1.6.0->-r requirements.txt (line 1)) (1.6.0)\n",
            "Requirement already satisfied: cssselect>=0.9 in /usr/local/lib/python3.6/dist-packages (from Scrapy==1.6.0->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: Twisted>=13.1.0 in /usr/local/lib/python3.6/dist-packages (from Scrapy==1.6.0->-r requirements.txt (line 1)) (20.3.0)\n",
            "Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.6/dist-packages (from Scrapy==1.6.0->-r requirements.txt (line 1)) (2.0.5)\n",
            "Requirement already satisfied: pyOpenSSL in /usr/local/lib/python3.6/dist-packages (from Scrapy==1.6.0->-r requirements.txt (line 1)) (19.1.0)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from Scrapy==1.6.0->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: soupsieve>=1.2 in /usr/local/lib/python3.6/dist-packages (from beautifulsoup4==4.8.0->-r requirements.txt (line 2)) (2.0.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->-r requirements.txt (line 3)) (2.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->-r requirements.txt (line 3)) (2020.4.5.1)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.6/dist-packages (from service-identity->Scrapy==1.6.0->-r requirements.txt (line 1)) (0.4.8)\n",
            "Requirement already satisfied: attrs>=16.0.0 in /usr/local/lib/python3.6/dist-packages (from service-identity->Scrapy==1.6.0->-r requirements.txt (line 1)) (19.3.0)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.6/dist-packages (from service-identity->Scrapy==1.6.0->-r requirements.txt (line 1)) (2.9.2)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.6/dist-packages (from service-identity->Scrapy==1.6.0->-r requirements.txt (line 1)) (0.2.8)\n",
            "Requirement already satisfied: Automat>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=13.1.0->Scrapy==1.6.0->-r requirements.txt (line 1)) (20.2.0)\n",
            "Requirement already satisfied: zope.interface>=4.4.2 in /usr/local/lib/python3.6/dist-packages (from Twisted>=13.1.0->Scrapy==1.6.0->-r requirements.txt (line 1)) (5.1.0)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=13.1.0->Scrapy==1.6.0->-r requirements.txt (line 1)) (15.1.0)\n",
            "Requirement already satisfied: PyHamcrest!=1.10.0,>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=13.1.0->Scrapy==1.6.0->-r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=13.1.0->Scrapy==1.6.0->-r requirements.txt (line 1)) (19.0.0)\n",
            "Requirement already satisfied: incremental>=16.10.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=13.1.0->Scrapy==1.6.0->-r requirements.txt (line 1)) (17.5.0)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography->service-identity->Scrapy==1.6.0->-r requirements.txt (line 1)) (1.14.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from zope.interface>=4.4.2->Twisted>=13.1.0->Scrapy==1.6.0->-r requirements.txt (line 1)) (47.1.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography->service-identity->Scrapy==1.6.0->-r requirements.txt (line 1)) (2.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsReszSfBH1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 2. Load the list of search sites and search terms\n",
        "\n",
        "# Import PyDrive and associated libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Download a file based on its file ID.\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "# \n",
        "# Get the website list file and load it to a dataframe\n",
        "file_id = '1y8FL1rBu8yb2HEn7-ok-ILy1Wq3UrDZU'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('website_list.csv')\n",
        "website_list_df = pd.read_csv('website_list.csv')\n",
        "# Get the search term file and load it to a dataframe\n",
        "file_id = '18MyoEiAYTG6w6JRejv5d4qVK4RxYyRu2'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('search_terms.csv')\n",
        "search_terms_df = pd.read_csv('search_terms.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjL9glrrmEro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "domain = [s.replace('www.', '') for s in website_list_df.Website.to_list()]\n",
        "website_list_df['Domain'] = domain"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDx9rY5rb7hN",
        "colab_type": "text"
      },
      "source": [
        "#1. Modify the python scripts to target them at the desired websites\n",
        "Todolist:\n",
        " * [ ] edit the python to change the url to a passed variable the targeted site(s) with the desired targets\n",
        " * [ ]  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlHz633Ib5xo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO write a script that removes the generic sites from the imported python code with the desired target(s)\n",
        "#TODO "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb-e4zNwbTPJ",
        "colab_type": "text"
      },
      "source": [
        "#2. Run the spider script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PivlM_MTpkkr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ALLOWED_DOMAINS = [s.replace('www.', '') for s in website_list_df.Website.to_list()]\n",
        "URLS = website_list_df.Website.to_list()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdEKGk4ZqIKU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "29568ac4-8d68-4423-a2e3-4801d290d68a"
      },
      "source": [
        "print(URLS)\n",
        "print(ALLOWED_DOMAINS)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['www.miit.gov.cn', 'www.sastind.gov.cn', 'www.jmjh.miit.gov.cn']\n",
            "['miit.gov.cn', 'sastind.gov.cn', 'jmjh.miit.gov.cn']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7_T8LmUgZw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standard library imports one per line\n",
        "import sys\n",
        "from urllib.parse import urlparse, urljoin\n",
        "\n",
        "# Third party library imports one per line\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor\n",
        "from scrapy.spiders import Rule\n",
        "# Global variables\n",
        "\n",
        "visited = list()\n",
        "NAME = 'sixnineeight'\n",
        "\"\"\"ALLOWED_DOMAINS = [s.replace('www.', '') for s in website_list_df.Website.to_list()]\n",
        "URLS = website_list_df.Website.to_list()\"\"\"\n",
        "\n",
        "\n",
        "class SixnineeightSpider(scrapy.Spider):\n",
        "    \"\"\"\n",
        "        Summary:\n",
        "            `SixnineeightSpider` instantiates the custom spider object.\n",
        "            Parameters:\n",
        "                scrapy.Spider (obj):\n",
        "            Returns:\n",
        "                (None)\n",
        "    \"\"\"\n",
        "    name = NAME\n",
        "    allowed_domains = [ALLOWED_DOMAINS]\n",
        "    start_urls = [URLS]\n",
        "    custom_settings = {\n",
        "        'DEPTH_LIMIT': 1  # Gets deep quick...2 is very deep on large sites e.g. 45K+ unique links on CNN. 0=no limit\n",
        "    }\n",
        "    rules = (\n",
        "        Rule(\n",
        "            LxmlLinkExtractor(allow=()),\n",
        "            callback='parse_obj',\n",
        "            follow=True),\n",
        "    )\n",
        "\n",
        "    def parse(self, response):\n",
        "        \"\"\"\n",
        "            Summary:\n",
        "                `parse` method to parse spider responses.\n",
        "                Parameters:\n",
        "                    self (obj): self\n",
        "                    response (obj): scrapy.Response\n",
        "                Returns:\n",
        "                    (None)\n",
        "        \"\"\"\n",
        "        links = response.xpath('//a//@href').extract()\n",
        "        for link in links:\n",
        "            if \"#\" in link:\n",
        "                link = link.split(\"#\")[0]\n",
        "            if link in visited or urljoin(URLS,\n",
        "                                          link) in visited or \"mailto:\" in link or \"tel:\" in link or \"javascript:\" in link:\n",
        "                continue\n",
        "            else:\n",
        "                if ALLOWED_DOMAINS in link and link in urlparse(link).netloc:\n",
        "                    if urlparse(link).scheme == '':\n",
        "                        link = urljoin(urlparse(URLS).scheme, link)\n",
        "                    visited.append(link)\n",
        "                    yield {\n",
        "                        \"link\": link\n",
        "                    }\n",
        "                    continue\n",
        "                if urlparse(link).netloc == '' and link not in visited:\n",
        "                    visited.append(urljoin(URLS, link))\n",
        "                    yield {\n",
        "                        \"link\": urljoin(URLS, link)\n",
        "                    }\n",
        "                    continue\n",
        "                if urlparse(link).netloc != '' and ALLOWED_DOMAINS not in link:\n",
        "                    continue\n",
        "\n",
        "        for next_page in links:\n",
        "            if next_page is not None and urlparse(next_page).netloc == '' or ALLOWED_DOMAINS in next_page:\n",
        "                if urlparse(next_page).netloc == '':\n",
        "                    next_page = urljoin(URLS, next_page)\n",
        "                next_page = response.urljoin(next_page)\n",
        "                yield scrapy.Request(next_page, callback=self.parse, dont_filter=True)\n",
        "\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aPlE8OIg_B9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        },
        "outputId": "59b8475b-c995-48f0-8030-e165f5511230"
      },
      "source": [
        "process = CrawlerProcess(\n",
        "        settings={\n",
        "            # 'FEED_FORMAT': 'pickle',\n",
        "            # 'FEED_URI': 'file:///***/links.pkl',\n",
        "            # 'LOG_LEVEL': 'INFO',  # Uncomment if you don't want scrapy to puke DEBUG to the console\n",
        "            # 'DOWNLOAD_DELAY': 0.25,   # 250 ms of delay, default is random between 0.5 - 1.5\n",
        "            'TELNETCONSOLE_ENABLED': False,  # On by default...that's dumb ¯\\_(ツ)_/¯\n",
        "            'FEED_FORMAT': 'json',\n",
        "            'FEED_URI': 'links.json',\n",
        "            'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36'\n",
        "        }\n",
        "    )\n",
        "process.crawl(SixnineeightSpider)\n",
        "try:\n",
        "    process.start()\n",
        "    print(\"It worked!!\")\n",
        "    print(\"Output {} file written to {}.\".format(process.settings['FEED_FORMAT'], process.settings['FEED_URI']))\n",
        "    sys.exit()\n",
        "except Exception as e:\n",
        "    sys.exit(\"Well, that didn't work... \\n {}\".format(e))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-16 01:08:58 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: scrapybot)\n",
            "2020-06-16 01:08:58 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.6.9 (default, Apr 18 2020, 01:56:04) - [GCC 8.4.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 2.9.2, Platform Linux-4.19.104+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2020-06-16 01:08:58 [scrapy.crawler] INFO: Overridden settings: {'DEPTH_LIMIT': 1, 'FEED_FORMAT': 'json', 'FEED_URI': 'links.json', 'TELNETCONSOLE_ENABLED': False, 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36'}\n",
            "2020-06-16 01:08:58 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.feedexport.FeedExporter',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2020-06-16 01:08:58 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2020-06-16 01:08:58 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2020-06-16 01:08:58 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2020-06-16 01:08:58 [scrapy.core.engine] INFO: Spider opened\n",
            "2020-06-16 01:08:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2020-06-16 01:08:58 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method OffsiteMiddleware.spider_opened of <scrapy.spidermiddlewares.offsite.OffsiteMiddleware object at 0x7f3d0dc789b0>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/twisted/internet/defer.py\", line 151, in maybeDeferred\n",
            "    result = f(*args, **kw)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pydispatch/robustapply.py\", line 55, in robustApply\n",
            "    return receiver(*arguments, **named)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scrapy/spidermiddlewares/offsite.py\", line 67, in spider_opened\n",
            "    self.host_regex = self.get_host_regex(spider)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scrapy/spidermiddlewares/offsite.py\", line 58, in get_host_regex\n",
            "    if url_pattern.match(domain):\n",
            "TypeError: expected string or bytes-like object\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Well, that didn't work... \n \n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIP2Zs1p1VVz",
        "colab_type": "code",
        "outputId": "d8b2f88c-74aa-4189-86ed-a6c4c8c9e126",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Option one for calling the file\n",
        "#pass the list of sites you want to scrape\n",
        "%cd {repo_dir}\n",
        "import sixnineeight"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/698S_BOG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9rQsKhKWoDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Option two for calling the file\n",
        "%cd /content/698S_BOG\n",
        "!python3 sixnineeight.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvcLopusbcvO",
        "colab_type": "text"
      },
      "source": [
        "#import data analysis libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o81AiAWqY3Tg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tX1Zr96Nbj_X",
        "colab_type": "code",
        "outputId": "b13e71d2-533a-4603-dca4-bce00f8c5e70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "pd.read_json('links.json')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.cnn.com/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.cnn.com/us</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://www.cnn.com/world</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://www.cnn.com/politics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://www.cnn.com/business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4344</th>\n",
              "      <td>https://www.cnn.com/health/video/2020/03/27/v8...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4345</th>\n",
              "      <td>https://www.cnn.com/travel/video/2020/03/27/v8...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4346</th>\n",
              "      <td>https://www.cnn.com/interactive/call-to-earth</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4347</th>\n",
              "      <td>https://www.cnn.com/specials/world/freedom-pro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4348</th>\n",
              "      <td>https://www.cnn.com/specials/africa/inside-africa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4349 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   link\n",
              "0                                  https://www.cnn.com/\n",
              "1                                https://www.cnn.com/us\n",
              "2                             https://www.cnn.com/world\n",
              "3                          https://www.cnn.com/politics\n",
              "4                          https://www.cnn.com/business\n",
              "...                                                 ...\n",
              "4344  https://www.cnn.com/health/video/2020/03/27/v8...\n",
              "4345  https://www.cnn.com/travel/video/2020/03/27/v8...\n",
              "4346      https://www.cnn.com/interactive/call-to-earth\n",
              "4347  https://www.cnn.com/specials/world/freedom-pro...\n",
              "4348  https://www.cnn.com/specials/africa/inside-africa\n",
              "\n",
              "[4349 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o5fZ0xSbqii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}